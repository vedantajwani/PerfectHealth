{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd4f354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load all 4 WHOOP exports\n",
    "sleep = pd.read_csv(\"../data/whoop/sleeps.csv\")\n",
    "workouts = pd.read_csv(\"../data/whoop/workouts.csv\")\n",
    "journal = pd.read_csv(\"../data/whoop/journal_entries.csv\")\n",
    "physio = pd.read_csv(\"../data/whoop/physiological_cycles.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d2ce79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Combined WHOOP dataset created with 158 days of data\n",
      "CSV: ../data/modified/whoop_combined.csv\n",
      "JSON: ../data/modified/whoop.json\n",
      "           date  recovery_score_pct  resting_hr_bpm  hrv_ms  skin_temp_c  \\\n",
      "157  2025-02-27                 NaN             NaN     NaN          NaN   \n",
      "156  2025-02-28                23.0            65.0    56.0        34.99   \n",
      "155  2025-02-28                23.0            65.0    56.0        34.99   \n",
      "154  2025-03-01                53.0            64.0    56.0        34.61   \n",
      "153  2025-03-02                71.0            62.0    69.0        34.70   \n",
      "\n",
      "     blood_oxygen_pct  day_strain          sleep_onset           wake_onset  \\\n",
      "157               NaN         9.3                  NaN                  NaN   \n",
      "156             97.00        17.8  2025-02-28 02:00:37  2025-02-28 09:44:56   \n",
      "155             97.00        17.8  2025-02-28 12:11:03  2025-02-28 15:41:06   \n",
      "154             96.34         9.3  2025-03-01 02:46:53  2025-03-01 11:54:16   \n",
      "153             95.95        10.8  2025-03-02 02:58:10  2025-03-02 11:02:17   \n",
      "\n",
      "     asleep_duration_min  ...  max_hr_bpm  avg_hr_bpm  \\\n",
      "157                  NaN  ...       143.0       108.0   \n",
      "156                440.0  ...       181.0       152.0   \n",
      "155                204.0  ...       181.0       152.0   \n",
      "154                479.0  ...       143.0       108.0   \n",
      "153                467.0  ...       164.0       117.0   \n",
      "\n",
      "     first_workout_start_time  last_workout_end_time  had_workout  \\\n",
      "157       2025-02-27 20:46:24    2025-02-27 22:11:42         True   \n",
      "156       2025-02-28 18:31:28    2025-02-28 19:48:52         True   \n",
      "155       2025-02-28 18:31:28    2025-02-28 19:48:52         True   \n",
      "154       2025-03-01 22:27:43    2025-03-01 23:46:02         True   \n",
      "153       2025-03-02 21:36:20    2025-03-02 22:33:02         True   \n",
      "\n",
      "     had_alcohol  had_caffeine  ate_late  used_marijuana  used_melatonin  \n",
      "157        False         False     False           False           False  \n",
      "156        False         False     False           False           False  \n",
      "155        False         False     False           False           False  \n",
      "154        False         False     False           False           False  \n",
      "153        False         False      True           False           False  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jx/5bn9zvwj6clbqrkgp6zc59s00000gn/T/ipykernel_54810/3257286808.py:130: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  journal_wide[col] = journal_wide[col].fillna(False).astype(bool)\n",
      "/var/folders/jx/5bn9zvwj6clbqrkgp6zc59s00000gn/T/ipykernel_54810/3257286808.py:130: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  journal_wide[col] = journal_wide[col].fillna(False).astype(bool)\n",
      "/var/folders/jx/5bn9zvwj6clbqrkgp6zc59s00000gn/T/ipykernel_54810/3257286808.py:130: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  journal_wide[col] = journal_wide[col].fillna(False).astype(bool)\n",
      "/var/folders/jx/5bn9zvwj6clbqrkgp6zc59s00000gn/T/ipykernel_54810/3257286808.py:130: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  journal_wide[col] = journal_wide[col].fillna(False).astype(bool)\n",
      "/var/folders/jx/5bn9zvwj6clbqrkgp6zc59s00000gn/T/ipykernel_54810/3257286808.py:130: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  journal_wide[col] = journal_wide[col].fillna(False).astype(bool)\n",
      "/var/folders/jx/5bn9zvwj6clbqrkgp6zc59s00000gn/T/ipykernel_54810/3257286808.py:158: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.fillna({\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Add a 'date' column based on WHOOP's cycle start time\n",
    "for df in (sleep, workouts, journal, physio):\n",
    "    df[\"date\"] = pd.to_datetime(df[\"Cycle start time\"]).dt.date\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1) SLEEP FEATURES  (from sleeps.csv)\n",
    "# ----------------------------------------------------\n",
    "sleep_cols = [\n",
    "    \"date\",\n",
    "    \"Sleep onset\",\n",
    "    \"Wake onset\",\n",
    "    \"Asleep duration (min)\",\n",
    "    \"In bed duration (min)\",\n",
    "    \"Light sleep duration (min)\",\n",
    "    \"Deep (SWS) duration (min)\",\n",
    "    \"REM duration (min)\",\n",
    "    \"Awake duration (min)\",\n",
    "    \"Sleep performance %\",\n",
    "    \"Sleep efficiency %\",\n",
    "    \"Sleep need (min)\",\n",
    "    \"Sleep debt (min)\",\n",
    "    \"Respiratory rate (rpm)\",\n",
    "    \"Nap\",\n",
    "]\n",
    "\n",
    "sleep_feat = sleep[sleep_cols].rename(columns={\n",
    "    \"Sleep onset\": \"sleep_onset\",\n",
    "    \"Wake onset\": \"wake_onset\",\n",
    "    \"Asleep duration (min)\": \"asleep_duration_min\",\n",
    "    \"In bed duration (min)\": \"in_bed_duration_min\",\n",
    "    \"Light sleep duration (min)\": \"light_sleep_duration_min\",\n",
    "    \"Deep (SWS) duration (min)\": \"deep_sleep_duration_min\",\n",
    "    \"REM duration (min)\": \"rem_duration_min\",\n",
    "    \"Awake duration (min)\": \"awake_duration_min\",\n",
    "    \"Sleep performance %\": \"sleep_performance_pct\",\n",
    "    \"Sleep efficiency %\": \"sleep_efficiency_pct\",\n",
    "    \"Sleep need (min)\": \"sleep_need_min\",\n",
    "    \"Sleep debt (min)\": \"sleep_debt_min\",\n",
    "    \"Respiratory rate (rpm)\": \"respiratory_rate_rpm\",\n",
    "    \"Nap\": \"nap\",\n",
    "})\n",
    "sleep_feat[\"sleep_hours\"] = sleep_feat[\"asleep_duration_min\"] / 60.0\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2) PHYSIOLOGICAL FEATURES  (from physiological_cycles.csv)\n",
    "# ----------------------------------------------------\n",
    "physio_cols = [\n",
    "    \"date\",\n",
    "    \"Recovery score %\",\n",
    "    \"Resting heart rate (bpm)\",\n",
    "    \"Heart rate variability (ms)\",\n",
    "    \"Skin temp (celsius)\",\n",
    "    \"Blood oxygen %\",\n",
    "    \"Day Strain\",\n",
    "]\n",
    "\n",
    "physio_feat = physio[physio_cols].rename(columns={\n",
    "    \"Recovery score %\": \"recovery_score_pct\",\n",
    "    \"Resting heart rate (bpm)\": \"resting_hr_bpm\",\n",
    "    \"Heart rate variability (ms)\": \"hrv_ms\",\n",
    "    \"Skin temp (celsius)\": \"skin_temp_c\",\n",
    "    \"Blood oxygen %\": \"blood_oxygen_pct\",\n",
    "    \"Day Strain\": \"day_strain\",\n",
    "})\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3) WORKOUT FEATURES (daily)  (from workouts.csv)\n",
    "# ----------------------------------------------------\n",
    "workouts_daily = (\n",
    "    workouts.groupby(\"date\", as_index=False)\n",
    "    .agg({\n",
    "        \"Duration (min)\": \"sum\",           # total duration per day\n",
    "        \"Activity Strain\": \"mean\",         # avg strain per day\n",
    "        \"Energy burned (cal)\": \"sum\",      # total calories per day\n",
    "        \"Max HR (bpm)\": \"max\",             # max HR per day\n",
    "        \"Average HR (bpm)\": \"mean\",        # avg HR per day\n",
    "        \"Workout start time\": \"min\",       # first workout start\n",
    "        \"Workout end time\": \"max\",         # last workout end\n",
    "    })\n",
    "    .rename(columns={\n",
    "        \"Duration (min)\": \"total_workout_minutes\",\n",
    "        \"Activity Strain\": \"avg_activity_strain\",\n",
    "        \"Energy burned (cal)\": \"total_workout_energy_burned_cal\",\n",
    "        \"Max HR (bpm)\": \"max_hr_bpm\",\n",
    "        \"Average HR (bpm)\": \"avg_hr_bpm\",\n",
    "        \"Workout start time\": \"first_workout_start_time\",\n",
    "        \"Workout end time\": \"last_workout_end_time\",\n",
    "    })\n",
    ")\n",
    "\n",
    "workouts_daily[\"had_workout\"] = workouts_daily[\"total_workout_minutes\"] > 0\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4) JOURNAL FEATURES (alcohol, weed, caffeine, melatonin, late food)\n",
    "#     from journal_entries.csv\n",
    "# ----------------------------------------------------\n",
    "# Pivot so each question becomes a column\n",
    "journal_wide = (\n",
    "    journal.pivot_table(\n",
    "        index=\"date\",\n",
    "        columns=\"Question text\",\n",
    "        values=\"Answered yes\",\n",
    "        aggfunc=\"first\",\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Map original question text ‚Üí simpler column names\n",
    "journal_wide = journal_wide.rename(columns={\n",
    "    \"Ate food close to bedtime?\": \"ate_food_close_to_bedtime\",\n",
    "    \"Consumed caffeine?\": \"consumed_caffeine\",\n",
    "    \"Have any alcoholic drinks?\": \"have_any_alcoholic_drinks\",\n",
    "    \"Took a melatonin supplement?\": \"took_a_melatonin_supplement\",\n",
    "    \"Used marijuana?\": \"used_marijuana\",\n",
    "})\n",
    "\n",
    "# Build boolean flags\n",
    "for col in [\n",
    "    \"ate_food_close_to_bedtime\",\n",
    "    \"consumed_caffeine\",\n",
    "    \"have_any_alcoholic_drinks\",\n",
    "    \"took_a_melatonin_supplement\",\n",
    "    \"used_marijuana\",\n",
    "]:\n",
    "    if col in journal_wide.columns:\n",
    "        journal_wide[col] = journal_wide[col].fillna(False).astype(bool)\n",
    "    else:\n",
    "        journal_wide[col] = False\n",
    "\n",
    "journal_flags = journal_wide[[\n",
    "    \"date\",\n",
    "    \"have_any_alcoholic_drinks\",\n",
    "    \"consumed_caffeine\",\n",
    "    \"ate_food_close_to_bedtime\",\n",
    "    \"used_marijuana\",\n",
    "    \"took_a_melatonin_supplement\",\n",
    "]].rename(columns={\n",
    "    \"have_any_alcoholic_drinks\": \"had_alcohol\",\n",
    "    \"consumed_caffeine\": \"had_caffeine\",\n",
    "    \"ate_food_close_to_bedtime\": \"ate_late\",\n",
    "    \"used_marijuana\": \"used_marijuana\",\n",
    "    \"took_a_melatonin_supplement\": \"used_melatonin\",\n",
    "})\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 5) MERGE EVERYTHING BY DATE\n",
    "# ----------------------------------------------------\n",
    "df = physio_feat.merge(sleep_feat, on=\"date\", how=\"left\")\n",
    "df = df.merge(workouts_daily, on=\"date\", how=\"left\")\n",
    "df = df.merge(journal_flags, on=\"date\", how=\"left\")\n",
    "\n",
    "# Fill NaNs for booleans + workout metrics only\n",
    "df = df.fillna({\n",
    "    \"total_workout_minutes\": 0,\n",
    "    \"avg_activity_strain\": 0,\n",
    "    \"total_workout_energy_burned_cal\": 0,\n",
    "    \"max_hr_bpm\": 0,\n",
    "    \"avg_hr_bpm\": 0,\n",
    "    \"had_workout\": False,\n",
    "    \"had_alcohol\": False,\n",
    "    \"had_caffeine\": False,\n",
    "    \"ate_late\": False,\n",
    "    \"used_marijuana\": False,\n",
    "    \"used_melatonin\": False,\n",
    "})\n",
    "\n",
    "df = df.sort_values(\"date\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 6) EXPORT\n",
    "# ----------------------------------------------------\n",
    "out_csv = \"../data/modified/whoop_combined.csv\"\n",
    "out_json = \"../data/modified/whoop.json\"\n",
    "\n",
    "df.to_csv(out_csv, index=False)\n",
    "df.to_json(out_json, orient=\"records\", indent=2, date_format=\"iso\")\n",
    "\n",
    "print(f\"‚úÖ Combined WHOOP dataset created with {len(df)} days of data\")\n",
    "print(\"CSV:\", out_csv)\n",
    "print(\"JSON:\", out_json)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89d97954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required package if not already installed\n",
    "%pip install ics -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9bad11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found .ics files: ['vedantajwani@gmail.com.ics', 'vajwani5021@gmail.com.ics', 'vajwani@umass.edu.ics', 'Birthdays_vedantajwani@gmail.com.ics']\n",
      "üì• Reading vedantajwani@gmail.com.ics\n",
      "üì• Reading vajwani5021@gmail.com.ics\n",
      "üì• Reading vajwani@umass.edu.ics\n",
      "üì• Reading Birthdays_vedantajwani@gmail.com.ics\n",
      "‚úÖ Wrote 111 days to ../data/modified/calendar.json\n"
     ]
    }
   ],
   "source": [
    "from ics import Calendar\n",
    "from datetime import datetime, date, timedelta\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# CONFIG\n",
    "# ----------------------------------------------------\n",
    "CAL_DIR = Path(\"../data/calendars\")          # folder with all your .ics files\n",
    "OUTPUT_PATH = Path(\"../data/modified/calendar.json\") # output JSON\n",
    "\n",
    "# Match your WHOOP date range\n",
    "START_DATE = date(2025, 2, 28)\n",
    "END_DATE   = date(2025, 6, 18)\n",
    "\n",
    "\n",
    "def any_keyword(events, keywords):\n",
    "    \"\"\"Return True if ANY event title contains ANY of the keywords.\"\"\"\n",
    "    titles = [ (e[\"title\"] or \"\").lower() for e in events ]\n",
    "    return any(\n",
    "        any(k in title for k in keywords)\n",
    "        for title in titles\n",
    "    )\n",
    "\n",
    "\n",
    "def has_long_task_block(task_events, gap_threshold_min=15, block_min=120):\n",
    "    \"\"\"\n",
    "    Long task definition:\n",
    "      - any single task >= block_min minutes, OR\n",
    "      - cluster of tasks with gaps <= gap_threshold_min and\n",
    "        total continuous block length >= block_min minutes\n",
    "    \"\"\"\n",
    "    if not task_events:\n",
    "        return False\n",
    "\n",
    "    # If any single task is already >= block_min, we're done\n",
    "    if any(e[\"duration_min\"] >= block_min for e in task_events):\n",
    "        return True\n",
    "\n",
    "    # Sort by start time\n",
    "    sorted_tasks = sorted(task_events, key=lambda e: e[\"start\"])\n",
    "\n",
    "    # Parse datetimes\n",
    "    parsed = [\n",
    "        (\n",
    "            datetime.fromisoformat(e[\"start\"]),\n",
    "            datetime.fromisoformat(e[\"end\"])\n",
    "        )\n",
    "        for e in sorted_tasks\n",
    "    ]\n",
    "\n",
    "    # Merge into contiguous blocks with small gaps\n",
    "    blocks = []\n",
    "    current_start, current_end = parsed[0]\n",
    "\n",
    "    for s, e in parsed[1:]:\n",
    "        gap_min = (s - current_end).total_seconds() / 60.0\n",
    "        if gap_min <= gap_threshold_min:\n",
    "            # same block; extend end if needed\n",
    "            if e > current_end:\n",
    "                current_end = e\n",
    "        else:\n",
    "            # close previous block\n",
    "            blocks.append((current_start, current_end))\n",
    "            current_start, current_end = s, e\n",
    "\n",
    "    # last block\n",
    "    blocks.append((current_start, current_end))\n",
    "\n",
    "    # check block lengths\n",
    "    for s, e in blocks:\n",
    "        block_len_min = (e - s).total_seconds() / 60.0\n",
    "        if block_len_min >= block_min:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    ics_files = list(CAL_DIR.glob(\"*.ics\"))\n",
    "    if not ics_files:\n",
    "        print(f\"‚ö†Ô∏è No .ics files found in {CAL_DIR}. Put your calendar exports there.\")\n",
    "        return\n",
    "\n",
    "    print(\"Found .ics files:\", [f.name for f in ics_files])\n",
    "\n",
    "    events_by_day = {}\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 1) READ ALL CALENDARS & COLLECT EVENTS PER DAY\n",
    "    # ------------------------------------------------\n",
    "    for ics_file in ics_files:\n",
    "        print(f\"üì• Reading {ics_file.name}\")\n",
    "        with open(ics_file, \"r\") as f:\n",
    "            cal = Calendar(f.read())\n",
    "\n",
    "        for e in cal.events:\n",
    "            if e.begin is None or e.end is None:\n",
    "                continue\n",
    "\n",
    "            start_dt = e.begin.datetime\n",
    "            end_dt = e.end.datetime\n",
    "            event_date = start_dt.date()\n",
    "\n",
    "            # filter to WHOOP date range\n",
    "            if not (START_DATE <= event_date <= END_DATE):\n",
    "                continue\n",
    "\n",
    "            dur_min = int((end_dt - start_dt).total_seconds() / 60)\n",
    "            date_str = event_date.isoformat()\n",
    "\n",
    "            events_by_day.setdefault(date_str, []).append({\n",
    "                \"title\": (e.name or \"\").strip(),\n",
    "                \"start\": start_dt.isoformat(),\n",
    "                \"end\": end_dt.isoformat(),\n",
    "                \"duration_min\": dur_min,\n",
    "                \"source_calendar\": ics_file.name,\n",
    "            })\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 2) SUMMARIZE PER DAY\n",
    "    # ------------------------------------------------\n",
    "    calendar_summary = []\n",
    "\n",
    "    for date_str, events in events_by_day.items():\n",
    "        total_busy = sum(e[\"duration_min\"] for e in events)\n",
    "        num_events = len(events)\n",
    "\n",
    "        # late events (after 8pm)\n",
    "        num_late = 0\n",
    "        for e in events:\n",
    "            try:\n",
    "                hour = datetime.fromisoformat(e[\"start\"]).hour\n",
    "            except Exception:\n",
    "                hour = 0\n",
    "            if hour >= 20:\n",
    "                num_late += 1\n",
    "\n",
    "        # Flags via keywords\n",
    "        has_exam = any_keyword(events, [\"exam\", \"midterm\", \"quiz\", \"test\"])\n",
    "        has_workout = any_keyword(events, [\"gym\", \"workout\", \"lift\", \"run\", \"cardio\"])\n",
    "        has_interview = any_keyword(events, [\"interview\", \"screen\", \"onsite\"])\n",
    "\n",
    "        # Task-ish things (for has_task_due + long tasks)\n",
    "        task_keywords = [\n",
    "            \"assignment\", \"hw\", \"homework\", \"project\",\n",
    "            \"paper\", \"quiz\", \"exam\", \"due\"\n",
    "        ]\n",
    "        has_task_due = any_keyword(events, task_keywords)\n",
    "\n",
    "        # Extract only \"task\" events\n",
    "        task_events = []\n",
    "        for e in events:\n",
    "            title_lower = (e[\"title\"] or \"\").lower()\n",
    "            if any(k in title_lower for k in task_keywords):\n",
    "                task_events.append(e)\n",
    "\n",
    "        has_long_tasks = has_long_task_block(\n",
    "            task_events,\n",
    "            gap_threshold_min=15,  # \"little or no break\"\n",
    "            block_min=120          # 2 hours\n",
    "        )\n",
    "\n",
    "        calendar_summary.append({\n",
    "            \"date\": date_str,\n",
    "            \"total_busy_minutes\": total_busy,\n",
    "            \"num_events\": num_events,\n",
    "            \"num_late_events\": num_late,\n",
    "            \"has_exam\": has_exam,\n",
    "            \"has_workout\": has_workout,\n",
    "            \"has_interview\": has_interview,\n",
    "            \"has_task_due\": has_task_due,\n",
    "            \"has_long_tasks\": has_long_tasks,\n",
    "        })\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 3) FILL IN MISSING DAYS WITH ZEROS (OPTIONAL BUT NICE)\n",
    "    # ------------------------------------------------\n",
    "    # build lookup by date\n",
    "    by_date = {d[\"date\"]: d for d in calendar_summary}\n",
    "\n",
    "    all_days = []\n",
    "    curr = START_DATE\n",
    "    while curr <= END_DATE:\n",
    "        date_str = curr.isoformat()\n",
    "        if date_str in by_date:\n",
    "            all_days.append(by_date[date_str])\n",
    "        else:\n",
    "            all_days.append({\n",
    "                \"date\": date_str,\n",
    "                \"total_busy_minutes\": 0,\n",
    "                \"num_events\": 0,\n",
    "                \"num_late_events\": 0,\n",
    "                \"has_exam\": False,\n",
    "                \"has_workout\": False,\n",
    "                \"has_interview\": False,\n",
    "                \"has_task_due\": False,\n",
    "                \"has_long_tasks\": False,\n",
    "            })\n",
    "        curr += timedelta(days=1)\n",
    "\n",
    "    calendar_summary = sorted(all_days, key=lambda x: x[\"date\"])\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 4) WRITE OUTPUT\n",
    "    # ------------------------------------------------\n",
    "    OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(OUTPUT_PATH, \"w\") as f:\n",
    "        json.dump(calendar_summary, f, indent=2)\n",
    "\n",
    "    print(f\"‚úÖ Wrote {len(calendar_summary)} days to {OUTPUT_PATH}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8a5e81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final dataset ready: (158, 43)\n"
     ]
    }
   ],
   "source": [
    "whoop = pd.read_json(\"../data/modified/whoop.json\")\n",
    "calendar = pd.read_json(\"../data/modified/calendar.json\")\n",
    "\n",
    "whoop[\"date\"] = pd.to_datetime(whoop[\"date\"])\n",
    "calendar[\"date\"] = pd.to_datetime(calendar[\"date\"])\n",
    "\n",
    "final = whoop.merge(calendar, on=\"date\", how=\"left\")\n",
    "\n",
    "# Fill NaNs from calendar side (if any)\n",
    "for col in calendar.columns:\n",
    "    if col == \"date\":\n",
    "        continue\n",
    "    if final[col].dtype == bool or final[col].dtype == \"boolean\":\n",
    "        final[col] = final[col].fillna(False)\n",
    "    else:\n",
    "        final[col] = final[col].fillna(0)\n",
    "\n",
    "final.to_csv(\"../data/modified/final_dataset.csv\", index=False)\n",
    "final.to_json(\"../data/modified/final_dataset.json\", orient=\"records\", indent=2, date_format=\"iso\")\n",
    "\n",
    "print(\"‚úÖ Final dataset ready:\", final.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4d17896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 158\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 1. Load your original file (array of objects)\n",
    "with open(\"../data/modified/final_dataset.json\", \"r\") as f:\n",
    "    data = json.load(f)  # this should be a list of dicts\n",
    "\n",
    "print(type(data), len(data))  # optional sanity check\n",
    "\n",
    "# 2. Write as NDJSON (one JSON object per line)\n",
    "with open(\"../data/modified/final_data_nd.json\", \"w\") as f:\n",
    "    for row in data:\n",
    "        f.write(json.dumps(row) + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "perfecthealth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
